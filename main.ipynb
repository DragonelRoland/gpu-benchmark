{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "CUDA device count: 1\n",
      "CUDA device 0: NVIDIA GeForce RTX 3090\n",
      "CUDA device 0 capability: (8, 6)\n",
      "CUDA device 0 memory: 23.68 GB\n",
      "CUDA tensor operation successful!\n",
      "tensor([[0.6416, 0.2420, 0.5296],\n",
      "        [0.9849, 1.2160, 1.0862],\n",
      "        [0.8950, 1.0398, 0.5796],\n",
      "        [0.8477, 1.0390, 1.6287],\n",
      "        [1.1263, 0.4763, 1.0878]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA diagnostics\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"CUDA device count: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"CUDA device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"CUDA device {i} capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"CUDA device {i} memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU only.\")\n",
    "    \n",
    "# Test basic CUDA operation if available\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        x = torch.rand(5, 3).cuda()\n",
    "        y = torch.rand(5, 3).cuda()\n",
    "        z = x + y\n",
    "        print(\"CUDA tensor operation successful!\")\n",
    "        print(z)\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA tensor operation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA devices available: 1\n",
      "Device 0: NVIDIA GeForce RTX 3090\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA devices available: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/Workspace/Max/gpu-benchmark/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 13.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save log to: /home/deeplearning/Workspace/Max/gpu-benchmark/benchmark_2025-04-17_17-42-25.txt\n",
      "Successfully created log file\n",
      "Starting benchmark for 10 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.92it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.99it/s]00:02,  2.37s/it]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.94it/s]00:04,  2.36s/it]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.92it/s]00:07,  2.36s/it]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.91it/s]00:09,  2.37s/it]\n",
      "Generated: 5 imgs | Current temp: 67°C: : 5it [00:11,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BENCHMARK SUMMARY:\n",
      "Benchmark completed in 11.84 seconds\n",
      "Images generated: 5\n",
      "Images per second: 0.42\n",
      "Average GPU time per image: 2367.20 ms\n",
      "Total GPU processing time: 11.84 seconds\n",
      "GPU utilization: 99.9%\n",
      "\n",
      "Temperature Statistics:\n",
      "  Starting temperature: 59°C\n",
      "  Ending temperature: 68°C\n",
      "  Average temperature: 65.5°C\n",
      "  Maximum temperature: 68°C\n",
      "  Temperature increase: 9°C\n",
      "==================================================\n",
      "Log saved to /home/deeplearning/Workspace/Max/gpu-benchmark/benchmark_2025-04-17_17-42-25.txt\n",
      "Confirmed: log file exists at /home/deeplearning/Workspace/Max/gpu-benchmark/benchmark_2025-04-17_17-42-25.txt\n",
      "File size: 953 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pynvml\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Initialize NVIDIA Management Library for temperature monitoring\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Set up timing variables\n",
    "benchmark_duration = 10  # seconds (or 300 for 5 minutes)\n",
    "image_count = 0\n",
    "total_gpu_time = 0\n",
    "temp_readings = []\n",
    "\n",
    "# Get current timestamp and create log filename with absolute path\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "current_dir = os.getcwd()\n",
    "log_filename = os.path.join(current_dir, f\"benchmark_{timestamp}.txt\")\n",
    "\n",
    "# Print the log file path to verify\n",
    "print(f\"Will save log to: {log_filename}\")\n",
    "\n",
    "# Prompt for generation\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "# Try to create the log file with error handling\n",
    "try:\n",
    "    with open(log_filename, \"w\") as log:\n",
    "        log.write(f\"GPU Benchmark - {timestamp}\\n\")\n",
    "        log.write(f\"Device: {torch.cuda.get_device_name(0)}\\n\")\n",
    "        log.write(f\"Benchmark duration: {benchmark_duration} seconds\\n\")\n",
    "        log.write(f\"Prompt: {prompt}\\n\")\n",
    "        log.write(\"-\" * 50 + \"\\n\\n\")\n",
    "        log.write(\"DETAILED LOG:\\n\")\n",
    "    print(f\"Successfully created log file\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating log file: {e}\")\n",
    "    # Try creating in home directory as fallback\n",
    "    log_filename = os.path.expanduser(f\"~/benchmark_{timestamp}.txt\")\n",
    "    print(f\"Trying alternate location: {log_filename}\")\n",
    "    with open(log_filename, \"w\") as log:\n",
    "        log.write(f\"GPU Benchmark - {timestamp}\\n\")\n",
    "\n",
    "# Start the benchmark\n",
    "print(f\"Starting benchmark for {benchmark_duration} seconds...\")\n",
    "start_time = time.time()\n",
    "end_time = start_time + benchmark_duration\n",
    "\n",
    "# Function to safely append to log file\n",
    "def append_to_log(message):\n",
    "    try:\n",
    "        with open(log_filename, \"a\") as log:\n",
    "            log.write(message)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to log: {e}\")\n",
    "\n",
    "# Run until time is up\n",
    "with tqdm() as pbar:\n",
    "    while time.time() < end_time:\n",
    "        # Get GPU temperature and add to list\n",
    "        current_temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "        temp_readings.append(current_temp)\n",
    "        \n",
    "        # CUDA timing events\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        # Synchronize before generation\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Record start time\n",
    "        start_event.record()\n",
    "        \n",
    "        # Generate image (but don't save it)\n",
    "        image = pipe(prompt).images[0]\n",
    "        \n",
    "        # Record end time\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Calculate GPU time\n",
    "        gpu_time_ms = start_event.elapsed_time(end_event)\n",
    "        total_gpu_time += gpu_time_ms\n",
    "        \n",
    "        # Log this iteration\n",
    "        append_to_log(f\"Image {image_count}: Time={time.time()-start_time:.2f}s, Temp={current_temp}°C, GenTime={gpu_time_ms:.2f}ms\\n\")\n",
    "        \n",
    "        # Update counter and progress\n",
    "        image_count += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Generated: {image_count} imgs | Current temp: {current_temp}°C\")\n",
    "\n",
    "# Get final temperature reading\n",
    "final_temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "temp_readings.append(final_temp)\n",
    "\n",
    "# Calculate results\n",
    "elapsed = time.time() - start_time\n",
    "avg_time_ms = total_gpu_time / image_count if image_count > 0 else 0\n",
    "avg_temp = sum(temp_readings) / len(temp_readings)\n",
    "max_temp = max(temp_readings)\n",
    "\n",
    "# Create summary\n",
    "summary = \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "summary += \"BENCHMARK SUMMARY:\\n\"\n",
    "summary += f\"Benchmark completed in {elapsed:.2f} seconds\\n\"\n",
    "summary += f\"Images generated: {image_count}\\n\"\n",
    "summary += f\"Images per second: {image_count/elapsed:.2f}\\n\"\n",
    "summary += f\"Average GPU time per image: {avg_time_ms:.2f} ms\\n\"\n",
    "summary += f\"Total GPU processing time: {total_gpu_time/1000:.2f} seconds\\n\"\n",
    "summary += f\"GPU utilization: {(total_gpu_time/1000)/elapsed*100:.1f}%\\n\"\n",
    "summary += f\"\\nTemperature Statistics:\\n\"\n",
    "summary += f\"  Starting temperature: {temp_readings[0]}°C\\n\"\n",
    "summary += f\"  Ending temperature: {final_temp}°C\\n\"\n",
    "summary += f\"  Average temperature: {avg_temp:.1f}°C\\n\"\n",
    "summary += f\"  Maximum temperature: {max_temp}°C\\n\"\n",
    "summary += f\"  Temperature increase: {final_temp - temp_readings[0]}°C\\n\"\n",
    "summary += \"=\"*50\n",
    "\n",
    "# Print summary to console\n",
    "print(summary)\n",
    "\n",
    "# Add summary to log file\n",
    "append_to_log(summary)\n",
    "\n",
    "# Clean up\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "print(f\"Log saved to {log_filename}\")\n",
    "\n",
    "# Double check if file exists\n",
    "if os.path.exists(log_filename):\n",
    "    print(f\"Confirmed: log file exists at {log_filename}\")\n",
    "    print(f\"File size: {os.path.getsize(log_filename)} bytes\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find log file at {log_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alright then i have the benchmark part for SD diffusion done and tbh i should just set this to 5 minutes and this is enough, i dont need gemma as well i guess. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
