{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearning/Workspace/Max/gpu-benchmark/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 11.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save log to: /home/deeplearning/Workspace/Max/gpu-benchmark/benchmark_2025-04-18_19-48-30.txt\n",
      "Successfully created log file\n",
      "Starting benchmark for 10 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.83it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.41it/s]00:02,  2.45s/it]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.40it/s]00:04,  2.37s/it]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.40it/s]00:07,  2.35s/it]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.36it/s]00:09,  2.34s/it]\n",
      "Generated: 5 imgs | Current temp: 46°C: : 5it [00:11,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BENCHMARK SUMMARY:\n",
      "Benchmark completed in 11.74 seconds\n",
      "Images generated: 5\n",
      "Images per second: 0.43\n",
      "Average GPU time per image: 2347.77 ms\n",
      "Total GPU processing time: 11.74 seconds\n",
      "GPU utilization: 99.9%\n",
      "\n",
      "Temperature Statistics:\n",
      "  Starting temperature: 34°C\n",
      "  Ending temperature: 46°C\n",
      "  Average temperature: 42.8°C\n",
      "  Maximum temperature: 46°C\n",
      "  Temperature increase: 12°C\n",
      "==================================================\n",
      "Log saved to /home/deeplearning/Workspace/Max/gpu-benchmark/benchmark_2025-04-18_19-48-30.txt\n",
      "Confirmed: log file exists at /home/deeplearning/Workspace/Max/gpu-benchmark/benchmark_2025-04-18_19-48-30.txt\n",
      "File size: 954 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pynvml\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Initialize NVIDIA Management Library for temperature monitoring\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Set up timing variables\n",
    "benchmark_duration = 10  # seconds (or 300 for 5 minutes)\n",
    "image_count = 0\n",
    "total_gpu_time = 0\n",
    "temp_readings = []\n",
    "\n",
    "# Get current timestamp and create log filename with absolute path\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "current_dir = os.getcwd()\n",
    "log_filename = os.path.join(current_dir, f\"benchmark_{timestamp}.txt\")\n",
    "\n",
    "# Print the log file path to verify\n",
    "print(f\"Will save log to: {log_filename}\")\n",
    "\n",
    "# Prompt for generation\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "# Try to create the log file with error handling\n",
    "try:\n",
    "    with open(log_filename, \"w\") as log:\n",
    "        log.write(f\"GPU Benchmark - {timestamp}\\n\")\n",
    "        log.write(f\"Device: {torch.cuda.get_device_name(0)}\\n\")\n",
    "        log.write(f\"Benchmark duration: {benchmark_duration} seconds\\n\")\n",
    "        log.write(f\"Prompt: {prompt}\\n\")\n",
    "        log.write(\"-\" * 50 + \"\\n\\n\")\n",
    "        log.write(\"DETAILED LOG:\\n\")\n",
    "    print(f\"Successfully created log file\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating log file: {e}\")\n",
    "    # Try creating in home directory as fallback\n",
    "    log_filename = os.path.expanduser(f\"~/benchmark_{timestamp}.txt\")\n",
    "    print(f\"Trying alternate location: {log_filename}\")\n",
    "    with open(log_filename, \"w\") as log:\n",
    "        log.write(f\"GPU Benchmark - {timestamp}\\n\")\n",
    "\n",
    "# Start the benchmark\n",
    "print(f\"Starting benchmark for {benchmark_duration} seconds...\")\n",
    "start_time = time.time()\n",
    "end_time = start_time + benchmark_duration\n",
    "\n",
    "# Function to safely append to log file\n",
    "def append_to_log(message):\n",
    "    try:\n",
    "        with open(log_filename, \"a\") as log:\n",
    "            log.write(message)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to log: {e}\")\n",
    "\n",
    "# Run until time is up\n",
    "with tqdm() as pbar:\n",
    "    while time.time() < end_time:\n",
    "        # Get GPU temperature and add to list\n",
    "        current_temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "        temp_readings.append(current_temp)\n",
    "        \n",
    "        # CUDA timing events\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        # Synchronize before generation\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Record start time\n",
    "        start_event.record()\n",
    "        \n",
    "        # Generate image (but don't save it)\n",
    "        image = pipe(prompt).images[0]\n",
    "        \n",
    "        # Record end time\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Calculate GPU time\n",
    "        gpu_time_ms = start_event.elapsed_time(end_event)\n",
    "        total_gpu_time += gpu_time_ms\n",
    "        \n",
    "        # Log this iteration\n",
    "        append_to_log(f\"Image {image_count}: Time={time.time()-start_time:.2f}s, Temp={current_temp}°C, GenTime={gpu_time_ms:.2f}ms\\n\")\n",
    "        \n",
    "        # Update counter and progress\n",
    "        image_count += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Generated: {image_count} imgs | Current temp: {current_temp}°C\")\n",
    "\n",
    "# Get final temperature reading\n",
    "final_temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "temp_readings.append(final_temp)\n",
    "\n",
    "# Calculate results\n",
    "elapsed = time.time() - start_time\n",
    "avg_time_ms = total_gpu_time / image_count if image_count > 0 else 0\n",
    "avg_temp = sum(temp_readings) / len(temp_readings)\n",
    "max_temp = max(temp_readings)\n",
    "\n",
    "# Create summary\n",
    "summary = \"\\n\" + \"=\"*50 + \"\\n\"\n",
    "summary += \"BENCHMARK SUMMARY:\\n\"\n",
    "summary += f\"Benchmark completed in {elapsed:.2f} seconds\\n\"\n",
    "summary += f\"Images generated: {image_count}\\n\"\n",
    "summary += f\"Images per second: {image_count/elapsed:.2f}\\n\"\n",
    "summary += f\"Average GPU time per image: {avg_time_ms:.2f} ms\\n\"\n",
    "summary += f\"Total GPU processing time: {total_gpu_time/1000:.2f} seconds\\n\"\n",
    "summary += f\"GPU utilization: {(total_gpu_time/1000)/elapsed*100:.1f}%\\n\"\n",
    "summary += f\"\\nTemperature Statistics:\\n\"\n",
    "summary += f\"  Starting temperature: {temp_readings[0]}°C\\n\"\n",
    "summary += f\"  Ending temperature: {final_temp}°C\\n\"\n",
    "summary += f\"  Average temperature: {avg_temp:.1f}°C\\n\"\n",
    "summary += f\"  Maximum temperature: {max_temp}°C\\n\"\n",
    "summary += f\"  Temperature increase: {final_temp - temp_readings[0]}°C\\n\"\n",
    "summary += \"=\"*50\n",
    "\n",
    "# Print summary to console\n",
    "print(summary)\n",
    "\n",
    "# Add summary to log file\n",
    "append_to_log(summary)\n",
    "\n",
    "# Clean up\n",
    "pynvml.nvmlShutdown()\n",
    "\n",
    "print(f\"Log saved to {log_filename}\")\n",
    "\n",
    "# Double check if file exists\n",
    "if os.path.exists(log_filename):\n",
    "    print(f\"Confirmed: log file exists at {log_filename}\")\n",
    "    print(f\"File size: {os.path.getsize(log_filename)} bytes\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find log file at {log_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Supabase credentials loaded successfully\n",
      "Uploading the following data to Supabase:\n",
      "  created_at: 2025-04-18T19:48:42.985015\n",
      "  gpu_type: NVIDIA GeForce RTX 3090\n",
      "  number_images_generated: 5\n",
      "  max_heat: 46\n",
      "  avg_heat: 42\n",
      "  country: 🇩🇪\n",
      "\n",
      "Connecting to Supabase...\n",
      "Uploading results...\n",
      "✅ Results successfully uploaded to Supabase! 🇩🇪\n",
      "Inserted record ID: 6\n"
     ]
    }
   ],
   "source": [
    "# Install the Supabase client if needed\n",
    "# !pip install supabase requests\n",
    "\n",
    "from supabase import create_client\n",
    "import datetime\n",
    "import platform\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Supabase credentials from environment variables\n",
    "supabase_url = os.getenv(\"SUPABASE_URL\")\n",
    "supabase_key = os.getenv(\"SUPABASE_KEY\")\n",
    "\n",
    "# Verify credentials were loaded\n",
    "if not supabase_url or not supabase_key:\n",
    "    print(\"❌ Error: Supabase credentials not found in .env file\")\n",
    "    print(\"Make sure you have a .env file with SUPABASE_URL and SUPABASE_KEY\")\n",
    "else:\n",
    "    print(f\"✅ Supabase credentials loaded successfully\")\n",
    "\n",
    "# Convert country code to flag emoji (Unicode regional indicator symbols)\n",
    "def country_code_to_flag(country_code):\n",
    "    if len(country_code) != 2 or not country_code.isalpha():\n",
    "        return \"🏳️\"  # White flag for unknown\n",
    "    \n",
    "    # Convert each letter to regional indicator symbol\n",
    "    # A-Z: 0x41-0x5A -> regional indicators: 0x1F1E6-0x1F1FF\n",
    "    return ''.join(chr(ord(c.upper()) - ord('A') + ord('🇦')) for c in country_code)\n",
    "\n",
    "# Get country information\n",
    "try:\n",
    "    country_response = requests.get(\"https://ipinfo.io/json\")\n",
    "    country_code = country_response.json().get(\"country\", \"Unknown\")\n",
    "    \n",
    "    # Convert country code to flag emoji\n",
    "    flag_emoji = country_code_to_flag(country_code)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting country info: {e}\")\n",
    "    flag_emoji = \"🏳️\"  # White flag for unknown\n",
    "\n",
    "# Prepare benchmark results for Supabase using data from the previous run\n",
    "benchmark_results = {\n",
    "    \"created_at\": datetime.datetime.now().isoformat(),\n",
    "    \"gpu_type\": torch.cuda.get_device_name(0),\n",
    "    \"number_images_generated\": image_count,\n",
    "    \"max_heat\": int(max_temp),\n",
    "    \"avg_heat\": int(avg_temp),\n",
    "    \"country\": flag_emoji  # Store the flag emoji directly\n",
    "}\n",
    "\n",
    "# Print the data being uploaded\n",
    "print(\"Uploading the following data to Supabase:\")\n",
    "for key, value in benchmark_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Upload to Supabase\n",
    "try:\n",
    "    print(\"\\nConnecting to Supabase...\")\n",
    "    # Initialize Supabase client\n",
    "    supabase = create_client(supabase_url, supabase_key)\n",
    "    \n",
    "    print(\"Uploading results...\")\n",
    "    # Insert benchmark results into the 'benchmark' table\n",
    "    response = supabase.table('benchmark').insert(benchmark_results).execute()\n",
    "    \n",
    "    # Check for successful upload\n",
    "    if hasattr(response, 'data') and response.data:\n",
    "        print(f\"✅ Results successfully uploaded to Supabase! {flag_emoji}\")\n",
    "        print(f\"Inserted record ID: {response.data[0]['id'] if response.data and len(response.data) > 0 else 'unknown'}\")\n",
    "    else:\n",
    "        print(\"❌ Error uploading to Supabase.\")\n",
    "        if hasattr(response, 'error'):\n",
    "            print(f\"Error details: {response.error}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error uploading to Supabase: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check that your Supabase URL and API key are correct in the .env file\")\n",
    "    print(\"2. Verify your network connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "what is the next thing to do now? someone installs the package and then should be able to run the program via the command line\n",
    "\n",
    "next i need to change the logs which are visible in the terminal and i need to change the logs which are written to the\n",
    "log file - question is how can i achieve this by \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
