{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "CUDA device count: 1\n",
      "CUDA device 0: NVIDIA GeForce RTX 3090\n",
      "CUDA device 0 capability: (8, 6)\n",
      "CUDA device 0 memory: 23.68 GB\n",
      "CUDA tensor operation successful!\n",
      "tensor([[0.6416, 0.2420, 0.5296],\n",
      "        [0.9849, 1.2160, 1.0862],\n",
      "        [0.8950, 1.0398, 0.5796],\n",
      "        [0.8477, 1.0390, 1.6287],\n",
      "        [1.1263, 0.4763, 1.0878]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA diagnostics\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"CUDA device count: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"CUDA device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"CUDA device {i} capability: {torch.cuda.get_device_capability(i)}\")\n",
    "        print(f\"CUDA device {i} memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU only.\")\n",
    "    \n",
    "# Test basic CUDA operation if available\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        x = torch.rand(5, 3).cuda()\n",
    "        y = torch.rand(5, 3).cuda()\n",
    "        z = x + y\n",
    "        print(\"CUDA tensor operation successful!\")\n",
    "        print(z)\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA tensor operation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA devices available: 1\n",
      "Device 0: NVIDIA GeForce RTX 3090\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA devices available: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 13.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Temperature: 36°C\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "# Initialize NVIDIA Management Library\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "# Get handle for GPU 0 (first GPU)\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "# Get GPU temperature in Celsius\n",
    "temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "\n",
    "# Print temperature\n",
    "print(f\"GPU Temperature: {temp}°C\")\n",
    "\n",
    "# Clean up\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 22.32it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "check how many images i was able to generate in 3 minutes\n",
    "\n",
    "temperature tracking - ie take temperature every second\n",
    "\"\"\"\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]  \n",
    "    \n",
    "image.save(\"astronaut_rides_horse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 22.23it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU execution time: 2319.58 ms (2.320 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Prompt for generation\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "# Create CUDA events for timing\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# Warmup run (to ensure any compilation/optimization happens before timing)\n",
    "_ = pipe(prompt).images[0]\n",
    "\n",
    "# Synchronize CUDA for accurate timing\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Start timing\n",
    "start_event.record()\n",
    "\n",
    "# Run the model\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "# End timing\n",
    "end_event.record()\n",
    "\n",
    "# Wait for GPU to finish\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Calculate elapsed time in milliseconds\n",
    "gpu_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "# Print results\n",
    "print(f\"GPU execution time: {gpu_time_ms:.2f} ms ({gpu_time_ms/1000:.3f} seconds)\")\n",
    "\n",
    "# Save image\n",
    "image.save(\"astronaut_rides_horse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
